[6/23 8:47 AM] PHILIPP Anne
    ok, so you'd like to have a real taught course 
[6/23 8:48 AM] PHILIPP Anne
    I would like that too
[6/23 8:48 AM] PHILIPP Anne
    are you interested in a specific programming language?
[6/23 8:49 AM] PHILIPP Anne
    MPI or OpenMP or GPU?
[6/23 8:50 AM] PHILIPP Anne
     I know about these two options:
[6/23 8:50 AM] PHILIPP Anne

https://www.hlrs.de/training/
http://typo3.vsc.ac.at/research/vsc-research-center/vsc-school-seminar/

[6/23 9:42 AM] HOFMAN Radek
    i do not know if somethig like that exists
?[6/23 9:43 AM] HOFMAN Radek

I am afraid that all, e.g. three days courses, will be ready for
various audiences and thus to be on the safe side start with some
elementary things like: what is a string, what is a number and in the
afternoon the last day it could be finally interesting

https://www.udemy.com/course/high-performance-computing-with-python-3x/?utm_source=adwords&utm_medium=udemyads&utm_campaign=Python_v.PROF_la.EN_cc.ROWMTA-A_ti.7380&utm_content=deal4584&utm_term=_._ag_77741653403_._ad_382048268635_._kw__._de_c_._dm__._pl__._ti_dsa-774930031889_._li_20044_._pd__._&matchtype=b&gclid=EAIaIQobChMI46q9sbiX6gIVxoGyCh2akQ2iEAAYAiAAEgLL5vD_BwE

well, this has 4 hours of videos and covers what KIANICKA Jan asked
Optimize performance and efficiency by leveraging NumPy, SciPy, and
Cython for numerical computations.
https://www.futurelearn.com/courses/python-in-hpc

# Records of the course
We use python3 (I used to work just with python2.6, python2.7)

Why is python slow:
1. Interpreted language with
2. Dynamic typing
3. Flexible data structures
4. Multithreading -
   Unfortunately, the memory management of the standard CPython
   interpreter is not thread-safe, and it uses something called Global
   Interpreter Lock (GIL) to safeguard memory integrity.

Fortunately, as we discuss in the course, many of the bottlenecks can
be circumvented.

As stated by famous computer scientist Donald Knuth already in the 70’s:
   Programmers waste enormous amounts of time thinking about, or
   worrying about, the speed of noncritical parts of their programs,
   and these attempts at efficiency actually have a strong negative
   impact when debugging and maintenance are considered: premature
   optimization is the root of all evil.

90% of time is spent in 10% of the source code.

Thus, it is clear that when optimizing one should focus only in the
time-critical parts of the program. How to find these hotspots is the
task of performance analysis.
- applications own timers
- performance analysis software

Some software by end of the execution provides own timers informaiton.
Bonus: timing with a context manager

Measuring very short execution times has some pitfalls that can be
avoided with the help of the 'timeit' Python module.

> python3 -m timeit -s "from math import sin" "sin(0.2)"
10000000 loops, best of 3: 0.0523 usec per loop

Inside interpreter:
>>> import timeit
>>> timeit.timeit('"-".join(str(n) for n in range(100))', number=10000)

Using of profiler cProfile:
> python3 -m cProfile -o profile.dat mandel_main.py
> python3 -m pstats profile.dat
# stripping of full path
profile.dat% strip
# sorting by time consumed
profile.dat% sort time
# show the 10m slowest functions
profile.dat% stats 10


For heat_main profiling we have this result:

profile.dat% stats 10
Thu Dec 10 17:12:46 2020    profile.dat

         449630 function calls (441871 primitive calls) in 23.728 seconds

   Ordered by: internal time
   List reduced from 3216 to 10 due to restriction <10>

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      200   22.455    0.112   22.455    0.112 /home/pythonuser/repo/hpc-python/performance/cprofile/heat.py:9(evolve)
      263    0.274    0.001    0.274    0.001 {method 'read' of '_io.FileIO' objects}
       24    0.189    0.008    0.191    0.008 {built-in method _imp.create_dynamic}
      263    0.060    0.000    0.060    0.000 {built-in method marshal.loads}
        2    0.044    0.022    0.044    0.022 {built-in method matplotlib._png.write_png}
    40000    0.037    0.000    0.041    0.000 /usr/lib/python3/dist-packages/numpy/lib/npyio.py:721(floatconv)
       12    0.035    0.003    0.035    0.003 {method 'readline' of '_io.BufferedReader' objects}
1147/1092    0.027    0.000    0.102    0.000 {built-in method builtins.__build_class__}
        8    0.022    0.003    0.023    0.003 {built-in method matplotlib._image.resample}
     3357    0.020    0.000    0.020    0.000 {built-in method numpy.core.multiarray.array}


profile.dat% stats 10
Thu Dec 10 17:12:46 2020    profile.dat

         449630 function calls (441871 primitive calls) in 23.728 seconds

   Ordered by: internal time
   List reduced from 3216 to 10 due to restriction <10>

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      200   22.455    0.112   22.455    0.112 /home/pythonuser/repo/hpc-python/performance/cprofile/heat.py:9(evolve)
      263    0.274    0.001    0.274    0.001 {method 'read' of '_io.FileIO' objects}
       24    0.189    0.008    0.191    0.008 {built-in method _imp.create_dynamic}
      263    0.060    0.000    0.060    0.000 {built-in method marshal.loads}
        2    0.044    0.022    0.044    0.022 {built-in method matplotlib._png.write_png}
    40000    0.037    0.000    0.041    0.000 /usr/lib/python3/dist-packages/numpy/lib/npyio.py:721(floatconv)
       12    0.035    0.003    0.035    0.003 {method 'readline' of '_io.BufferedReader' objects}
1147/1092    0.027    0.000    0.102    0.000 {built-in method builtins.__build_class__}
        8    0.022    0.003    0.023    0.003 {built-in method matplotlib._image.resample}
     3357    0.020    0.000    0.020    0.000 {built-in method numpy.core.multiarray.array}


profile.dat% stats 10
Thu Dec 10 17:12:46 2020    profile.dat

         449630 function calls (441871 primitive calls) in 23.728 seconds

   Ordered by: internal time
   List reduced from 3216 to 10 due to restriction <10>

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      200   22.455    0.112   22.455    0.112 /home/pythonuser/repo/hpc-python/performance/cprofile/heat.py:9(evolve)
      263    0.274    0.001    0.274    0.001 {method 'read' of '_io.FileIO' objects}
       24    0.189    0.008    0.191    0.008 {built-in method _imp.create_dynamic}
      263    0.060    0.000    0.060    0.000 {built-in method marshal.loads}
        2    0.044    0.022    0.044    0.022 {built-in method matplotlib._png.write_png}
    40000    0.037    0.000    0.041    0.000 /usr/lib/python3/dist-packages/numpy/lib/npyio.py:721(floatconv)
       12    0.035    0.003    0.035    0.003 {method 'readline' of '_io.BufferedReader' objects}
1147/1092    0.027    0.000    0.102    0.000 {built-in method builtins.__build_class__}
        8    0.022    0.003    0.023    0.003 {built-in method matplotlib._image.resample}
     3357    0.020    0.000    0.020    0.000 {built-in method numpy.core.multiarray.array}


We hope that you remember the number one rule in performance optimization:
Measure before you optimize!

Few resources found during the execrise:
https://docs.python.org/3/library/timeit.html
http://people.uncw.edu/hermanr/pde1/NumHeatEqn.pdf
https://pythonfiles.wordpress.com/2017/08/24/hunting-performance-in-python-code-part-4/


Numpy Arrays:
      Creating arrays
        1. From existing data: x = numpy.array((1, 2, 3, 4)), implicit type, floating point [1, 2, 3.1, 4]
	Second argument is type:
	data = [[1, 2, 3], [4, 5, 6]]
	y = numpy.array(data, complex)
	print(y)
	# output: [[ 1.+0.j  2.+0.j  3.+0.j]
	#          [ 4.+0.j  5.+0.j  6.+0.j]]

	print(y.shape)
	# output: (2, 3)

	print(y.size)
	# output: 6

	2. Using helper function
	arange
	a = numpy.arange(10)
	print(a)
	# output: [0 1 2 3 4 5 6 7 8 9]

	b = numpy.linspace(-4.5, 4.5, 5)

	print(b)
	# output: [-4.5  -2.25  0.    2.25  4.5]

	c = numpy.zeros((4, 6), float)
	d = numpy.ones((2, 4))
	e = numpy.full((3, 2), 4.2)

	empty (whatever happened to be in the computer memory allocated to the array)

	dna = 'AAAGTCTGAC'
	c = numpy.array(dna, dtype='c')

     Accessing arrays
     	Using indexes
	data = numpy.array([[1, 2, 3], [4, 5, 6]])
	x = data[0,2]
	y = data[1,-2]

	Slicing
	a = numpy.arange(10)

	print(a[2:])
	# output: [2 3 4 5 6 7 8 9]

	print(a[:-1])
	# output: [0 1 2 3 4 5 6 7 8]

	print(a[1:7:2])
	# output: [1 3 5]

	b = numpy.zeros((4, 4))
	b[1:3, 1:3] = 2.0

	Interesting example from the exercise:
	checker = np.zeros((8,8))
	checker[::2, ::2] = 1 (every second element in every second row, collumn)
	checker[1::2, 1::2] = 1
	print(checker)

    Views and copies of arrays
    	a = np.arange(10)
	b = a              # reference, changing values in b changes a
	b = a.copy()       # true copy

	In contrast, slicing an array will create something called a view to the array.

	a = np.arange(10)
	c = a[1:4]         # view, changing c changes elements [1:4] of a
	c = a[1:4].copy()  # true copy of subarray


Element funcions:
-----
	The main thing to keep in mind is that most operations are done element-wise.

	a = numpy.array([1.0, 2.0, 3.0])
	b = 2.0

	print(a * b)
	# output: [ 2.  4.  6.]

	print(a + b)
	# output: [ 3.  4.  5.]

	print(a * a)
	# output: [ 1.  4.  9.]

	In many ways NumPy can be used as a drop-in replacement for the math module.

	import numpy, math
	a = numpy.linspace(-math.pi, math.pi, 8)
	numpy.sin(a)

Vectorised operations
	# brute force using a for loop
	arr = numpy.arange(1000)
	dif = numpy.zeros(999, int)
	for i in range(1, len(arr)):
	dif[i-1] = arr[i] - arr[i-1]

	can be re-written as a vectorised operation:

	# vectorised operation
	arr = numpy.arange(1000)
	dif = arr[1:] - arr[:-1]

Array manipulation and broadcasting:
      Reshape:
      a = numpy.array([[1, 2, 3], [4, 5, 6]])

      print(a.shape)
      # output: (2,3)

      b = a.reshape(3,2)

      print(b)
      # output: [[1 2],
      #          [3 4],
      #          [5 6]]

      method called ravel() to flatten an array into 1D
      c = a.ravel()

      Concatenate
      a = numpy.array([[1, 2, 3], [4, 5, 6]])
      b = numpy.array([[7, 8, 9], [10, 11, 12]])
      c = numpy.concatenate((a, b))

      print(c)
      # output: [[ 1  2  3],
           	[ 4  5  6],
 		[ 7  8  9],
           	[10 11 12]]

      One can also specify the axis (dimension) along which the arrays will be joined:

      d = numpy.concatenate((a, b), axis=1)

      print(d)
      # output: [[ 1  2  3  7  8  9],
           	[ 4  5  6 10 11 12]]

      Split
      a = numpy.array([[1, 2, 3], [4, 5, 6]])
      x = numpy.split(a, 2)
      y = numpy.split(a, 3, axis=1)

      Broadcasting
      (E.g. when we multiply array with scalar)

      Example: A 3x2 array multiplied by a 1x2 array:
      a = numpy.arange(6).reshape(3,2)
      b = numpy.array([7,11], float).reshape(1,2)

      print(a)
      # output: [[0 1],
      #          [2 3],
      #          [4 5]]

      print(b)
      # output: [[ 7.  11.]]

      c = a * b

      print(c)
      # output: [[  0.  11.],
      #          [ 14.  33.],
      #          [ 28.  55.]]

# Loading data from txt files

  xy = numpy.loadtxt('xy-coordinates.dat')
  From two collumns txt file:
  x   y
  0.1 0.1
  -2 nan

  args = {
       'header': 'XY coordinates',
         'fmt': '%7.3f',
	   'delimiter': ','
  }
  numpy.savetxt('output.dat', xy, **args)

  # It is also possible to save binaries
  numpy.save('a_bin',a)

# Random numbers
 
   - random: uniform random numbers
   - normal: normal distribution
   - choice: random sample from given array

   a = numpy.random.random((2,2))
   b = numpy.random.choice(numpy.arange(4), 10)

# Linear algebra and polynomials

  dot, vdot
  linalg.eig, linalg.eigvals
  linalg.solve
  linalg.inv

  A = numpy.array(((2, 1), (1, 3)))
  B = numpy.array(((-2, 4.2), (4.2, 6)))

  C = numpy.dot(A, B)
  b = numpy.array((1, 2))

  print(C)
  # output:
  #   [[  0.2  14.4]
  #    [ 10.6  22.2]]

  print(b)
  # output: [1 2]

  # solve C x = b
  x = numpy.linalg.solve(C, b)

  print(x)
  # output: [ 0.04453441  0.06882591]

# Polynomials
  # f(x) = x^2 + random noise (between 0,1)
  x = numpy.linspace(-4, 4, 7)
  f = x**2 + numpy.random.random(x.shape)

  p = numpy.polyfit(x, f, 2)

  print(p)
  # output: [ 0.96869003  -0.01157275  0.69352514]
  #   f(x) =  p[0] * x^2 + p[1] * x  + p[2]


# Interesting information on indexing and numpy arrays characteristics

This is fast and creates the copy of the array -
finally I remembered how it was in GeoModel yet:

>>> a
    array([[ 2.3,  2.1],
       [ 4.2,  1.2]])

>>> ind = numpy.array([[0,1],[2,3]])
>>> a.flatten()[ind]
array([[ 2.3,  2.1],
       [ 4.2,  1.2]])

n addition to slicing NumPy allows indexing also
with integer arrays or Boolean masks:


     a = np.arange(0.0, 1.0, 0.1)
     ind = np.array([1, 1, 0, 4])
     b = a[ind] # b = array([0.1, 0.1, 0. , 0.4])

     m = a > 0.5
     b = a[m] # b = array([0.6, 0.7, 0.8, 0.9])

In these cases b cannot be created just by modifying the strides,
so b will hold a copy of the data in a.
Now, modifications of b are not affecting a.

>>> a.flags
  C_CONTIGUOUS : True
  F_CONTIGUOUS : False
  OWNDATA : True
  WRITEABLE : True
  ALIGNED : True
  UPDATEIFCOPY : False
>>> a.strides
(16, 8)
>>> a
array([[ 2.3,  2.1],
       [ 4.2,  1.2]])
>>> a.itemsize
8
>>> a.data
<memory at 0x7f535a528480>
>>> a.__array_interface__
{'data': (12984000, False), 'strides': None, 'descr': [('', '<f8')], 'typestr': '<f8', 'shape': (2, 2), 'version': 3}

# What is Cython

Cython is an optimising static compiler for Python that also provides its own
programming language as a superset for standard Python.
Cython makes it also easy to interact with external C/C++ code.

The main mechanism of how Cython speeds up Python programs is by
adding static declarations for variables.

Operations on lists and dictionaries do not normally benefit much from Cython

In summary, Cython can alleviate the following Python performance bottlenecks discussed in Week 1:

    - Interpretation overhead
    - Unboxing / boxing Python objects
    - Overheads of Python control structures
    - Function call overheads


# Speed up python by using static typing

Strong typing means that variables do have a type and that the type
matters when performing operations on a variable.

Dynamic typing means that the type of the variable is
determined only during runtime.

Everything is an object.

The fact that everything is an object means that there is a lot of “unboxing”
and “boxing” involved when Python performs operations with variables.

when this is executed
c = a + b
there are several steps Python needs to do:

    Check the types of both operands
    Check whether they both support the + operation
    Extract the function that performs the + operation (due to operator
    overloading objects can have a custom definition for addition)
    Extract the actual values of the objects
    Perform the + operation
    Construct a new integer object for the result

What if one knows that e.g. in a certain function the variables have always
the same type? That’s where Cython steps in:

When Cythonizing a Python code, static type information can be added either:

    In function signatures by prefixing the formal arguments by their type
    By declaring variables with the cdef Cython keyword, followed by the
    the type

def add (int x, int y):
 cdef int result
 result = x + y
 return result


+--------------------+----------------+
|From Python types   |   To C types   |
|int                 |   int, long    |
|int, float          |   float, double|
|str/bytes           |   char *       |
+--------------------+----------------+


+-------------------------------+
|From C types   |To Python types|
|int, long      |int            |
|float, double  |float          |
|char*          |str/bytes      |
+-------------------------------+

Mandelbrot fractal kernel:

def kernel(zr, zi, cr, ci, lim, cutoff):
 ''' Computes the number of iterations `n` such that
 |z_n| > `lim`, where `z_n = z_{n-1}**2 + c`.
 '''
 count = 0
 while ((zr*zr + zi*zi) < (lim*lim)) and count < cutoff:
 zr, zi = zr * zr - zi * zi + cr, 2 * zr * zi + ci
 count += 1
 return count

Cythonized

def kernel(double zr, double zi, double cr, double ci, double lim, int cutoff):
 ''' Computes the number of iterations `n` such that
 |z_n| > `lim`, where `z_n = z_{n-1}**2 + c`.
 '''
 cdef int count = 0
 while ((zr*zr + zi*zi) < (lim*lim)) and count < cutoff:
 zr, zi = zr * zr - zi * zi + cr, 2 * zr * zi + ci
 count += 1
 return count

    Pure Python: 0.57 s
    Static type declarations in the kernel: 14 ms

Thus, we obtained a speed up of ~40 !

Link to Cyrhon types:
https://cython.readthedocs.io/en/latest/src/userguide/language_basics.html

# Creating Cython modules
Normally, when working with Cython one does not Cythonize the whole program
but only selected modules.

my_module.py would be
made into my_module.pyx.

Cython easier, one does not
normally use Cython directly from the command line but via Python distutils
setup.py build file:

from distutils.core import setup
from Cython.Build import cythonize

setup(
 ext_modules=cythonize("my_module.pyx"),
)

# Hands on training - static typing
- input arguments must match the return result type
How to run it:
- modify the file: cython/static-typing/cyt_module.pyx
Build:
> python3 setup.py cyt_module.pyx --inplace
Use:
> python3
>>> import cyt_module
>>> cyt_module.subtract(10.2, 8.5)
1.6999998092651367

# Avoid functuion call overhead
Python is slow because each function is using 'boxing' and 'unboxing'.
Just wrapping of function in function in python it makes performance
3-4 times slower.

We have two options:
1. Function used only in Cython:

   cdef int add (int x, int y):

2. Function used also from Python (about 1.5 slowler then pure cython):

   cpdef int add (int x, int y):

# Hands on exercise - Fibonaci to increase the speed
Fro fibonaci we need to build it like this:
> python3 setup.py build_ext --inplace
(Using fib.pyx as argument does not work and build command creates the 'build' directory)

pythonuser@ubuntu:~/repo/hpc-python/cython/c-functions/solution$ python3 test_fib.py 
Pure Python:          0.4531 s
Cython:               5.0246 ms
Speedup:               90.2
Pure Python cached:   0.0924 us
Speedup over Cython:  5.4e+04


TODO - Next we go for Numpy with Cython


