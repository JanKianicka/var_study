[6/23 8:47 AM] PHILIPP Anne
    ok, so you'd like to have a real taught course 
[6/23 8:48 AM] PHILIPP Anne
    I would like that too
[6/23 8:48 AM] PHILIPP Anne
    are you interested in a specific programming language?
[6/23 8:49 AM] PHILIPP Anne
    MPI or OpenMP or GPU?
[6/23 8:50 AM] PHILIPP Anne
     I know about these two options:
[6/23 8:50 AM] PHILIPP Anne

https://www.hlrs.de/training/
http://typo3.vsc.ac.at/research/vsc-research-center/vsc-school-seminar/

[6/23 9:42 AM] HOFMAN Radek
    i do not know if somethig like that exists
?[6/23 9:43 AM] HOFMAN Radek

I am afraid that all, e.g. three days courses, will be ready for
various audiences and thus to be on the safe side start with some
elementary things like: what is a string, what is a number and in the
afternoon the last day it could be finally interesting

https://www.udemy.com/course/high-performance-computing-with-python-3x/?utm_source=adwords&utm_medium=udemyads&utm_campaign=Python_v.PROF_la.EN_cc.ROWMTA-A_ti.7380&utm_content=deal4584&utm_term=_._ag_77741653403_._ad_382048268635_._kw__._de_c_._dm__._pl__._ti_dsa-774930031889_._li_20044_._pd__._&matchtype=b&gclid=EAIaIQobChMI46q9sbiX6gIVxoGyCh2akQ2iEAAYAiAAEgLL5vD_BwE

well, this has 4 hours of videos and covers what KIANICKA Jan asked
Optimize performance and efficiency by leveraging NumPy, SciPy, and
Cython for numerical computations.
https://www.futurelearn.com/courses/python-in-hpc

# Records of the course
We use python3 (I used to work just with python2.6, python2.7)

Why is python slow:
1. Interpreted language with
2. Dynamic typing
3. Flexible data structures
4. Multithreading -
   Unfortunately, the memory management of the standard CPython
   interpreter is not thread-safe, and it uses something called Global
   Interpreter Lock (GIL) to safeguard memory integrity.

Fortunately, as we discuss in the course, many of the bottlenecks can
be circumvented.

As stated by famous computer scientist Donald Knuth already in the 70â€™s:
   Programmers waste enormous amounts of time thinking about, or
   worrying about, the speed of noncritical parts of their programs,
   and these attempts at efficiency actually have a strong negative
   impact when debugging and maintenance are considered: premature
   optimization is the root of all evil.

90% of time is spent in 10% of the source code.

Thus, it is clear that when optimizing one should focus only in the
time-critical parts of the program. How to find these hotspots is the
task of performance analysis.
- applications own timers
- performance analysis software

Some software by end of the execution provides own timers informaiton.
Bonus: timing with a context manager

Measuring very short execution times has some pitfalls that can be
avoided with the help of the 'timeit' Python module.

> python3 -m timeit -s "from math import sin" "sin(0.2)"
10000000 loops, best of 3: 0.0523 usec per loop

Inside interpreter:
>>> import timeit
>>> timeit.timeit('"-".join(str(n) for n in range(100))', number=10000)

Using of profiler cProfile:
> python3 -m cProfile -o profile.dat mandel_main.py
> python3 -m pstats profile.dat
# stripping of full path
profile.dat% strip
# sorting by time consumed
profile.dat% sort time
# show the 10m slowest functions
profile.dat% stats 10


For heat_main profiling we have this result:

profile.dat% stats 10
Thu Dec 10 17:12:46 2020    profile.dat

         449630 function calls (441871 primitive calls) in 23.728 seconds

   Ordered by: internal time
   List reduced from 3216 to 10 due to restriction <10>

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      200   22.455    0.112   22.455    0.112 /home/pythonuser/repo/hpc-python/performance/cprofile/heat.py:9(evolve)
      263    0.274    0.001    0.274    0.001 {method 'read' of '_io.FileIO' objects}
       24    0.189    0.008    0.191    0.008 {built-in method _imp.create_dynamic}
      263    0.060    0.000    0.060    0.000 {built-in method marshal.loads}
        2    0.044    0.022    0.044    0.022 {built-in method matplotlib._png.write_png}
    40000    0.037    0.000    0.041    0.000 /usr/lib/python3/dist-packages/numpy/lib/npyio.py:721(floatconv)
       12    0.035    0.003    0.035    0.003 {method 'readline' of '_io.BufferedReader' objects}
1147/1092    0.027    0.000    0.102    0.000 {built-in method builtins.__build_class__}
        8    0.022    0.003    0.023    0.003 {built-in method matplotlib._image.resample}
     3357    0.020    0.000    0.020    0.000 {built-in method numpy.core.multiarray.array}


profile.dat% stats 10
Thu Dec 10 17:12:46 2020    profile.dat

         449630 function calls (441871 primitive calls) in 23.728 seconds

   Ordered by: internal time
   List reduced from 3216 to 10 due to restriction <10>

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      200   22.455    0.112   22.455    0.112 /home/pythonuser/repo/hpc-python/performance/cprofile/heat.py:9(evolve)
      263    0.274    0.001    0.274    0.001 {method 'read' of '_io.FileIO' objects}
       24    0.189    0.008    0.191    0.008 {built-in method _imp.create_dynamic}
      263    0.060    0.000    0.060    0.000 {built-in method marshal.loads}
        2    0.044    0.022    0.044    0.022 {built-in method matplotlib._png.write_png}
    40000    0.037    0.000    0.041    0.000 /usr/lib/python3/dist-packages/numpy/lib/npyio.py:721(floatconv)
       12    0.035    0.003    0.035    0.003 {method 'readline' of '_io.BufferedReader' objects}
1147/1092    0.027    0.000    0.102    0.000 {built-in method builtins.__build_class__}
        8    0.022    0.003    0.023    0.003 {built-in method matplotlib._image.resample}
     3357    0.020    0.000    0.020    0.000 {built-in method numpy.core.multiarray.array}


profile.dat% stats 10
Thu Dec 10 17:12:46 2020    profile.dat

         449630 function calls (441871 primitive calls) in 23.728 seconds

   Ordered by: internal time
   List reduced from 3216 to 10 due to restriction <10>

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      200   22.455    0.112   22.455    0.112 /home/pythonuser/repo/hpc-python/performance/cprofile/heat.py:9(evolve)
      263    0.274    0.001    0.274    0.001 {method 'read' of '_io.FileIO' objects}
       24    0.189    0.008    0.191    0.008 {built-in method _imp.create_dynamic}
      263    0.060    0.000    0.060    0.000 {built-in method marshal.loads}
        2    0.044    0.022    0.044    0.022 {built-in method matplotlib._png.write_png}
    40000    0.037    0.000    0.041    0.000 /usr/lib/python3/dist-packages/numpy/lib/npyio.py:721(floatconv)
       12    0.035    0.003    0.035    0.003 {method 'readline' of '_io.BufferedReader' objects}
1147/1092    0.027    0.000    0.102    0.000 {built-in method builtins.__build_class__}
        8    0.022    0.003    0.023    0.003 {built-in method matplotlib._image.resample}
     3357    0.020    0.000    0.020    0.000 {built-in method numpy.core.multiarray.array}


We hope that you remember the number one rule in performance optimization:
Measure before you optimize!

Few resources found during the execrise:
https://docs.python.org/3/library/timeit.html
http://people.uncw.edu/hermanr/pde1/NumHeatEqn.pdf
https://pythonfiles.wordpress.com/2017/08/24/hunting-performance-in-python-code-part-4/


Numpy Arrays:
      Creating arrays
        1. From existing data: x = numpy.array((1, 2, 3, 4)), implicit type, floating point [1, 2, 3.1, 4]
	Second argument is type:
	data = [[1, 2, 3], [4, 5, 6]]
	y = numpy.array(data, complex)
	print(y)
	# output: [[ 1.+0.j  2.+0.j  3.+0.j]
	#          [ 4.+0.j  5.+0.j  6.+0.j]]

	print(y.shape)
	# output: (2, 3)

	print(y.size)
	# output: 6

	2. Using helper function
	arange
	a = numpy.arange(10)
	print(a)
	# output: [0 1 2 3 4 5 6 7 8 9]

	b = numpy.linspace(-4.5, 4.5, 5)

	print(b)
	# output: [-4.5  -2.25  0.    2.25  4.5]

	c = numpy.zeros((4, 6), float)
	d = numpy.ones((2, 4))
	e = numpy.full((3, 2), 4.2)

	empty (whatever happened to be in the computer memory allocated to the array)

	dna = 'AAAGTCTGAC'
	c = numpy.array(dna, dtype='c')

     Accessing arrays
     	Using indexes
	data = numpy.array([[1, 2, 3], [4, 5, 6]])
	x = data[0,2]
	y = data[1,-2]

	Slicing
	a = numpy.arange(10)

	print(a[2:])
	# output: [2 3 4 5 6 7 8 9]

	print(a[:-1])
	# output: [0 1 2 3 4 5 6 7 8]

	print(a[1:7:2])
	# output: [1 3 5]

	b = numpy.zeros((4, 4))
	b[1:3, 1:3] = 2.0

	Interesting example from the exercise:
	checker = np.zeros((8,8))
	checker[::2, ::2] = 1 (every second element in every second row, collumn)
	checker[1::2, 1::2] = 1
	print(checker)

    Views and copies of arrays
    	a = np.arange(10)
	b = a              # reference, changing values in b changes a
	b = a.copy()       # true copy

	In contrast, slicing an array will create something called a view to the array.

	a = np.arange(10)
	c = a[1:4]         # view, changing c changes elements [1:4] of a
	c = a[1:4].copy()  # true copy of subarray


Element funcions:
-----
	The main thing to keep in mind is that most operations are done element-wise.

	a = numpy.array([1.0, 2.0, 3.0])
	b = 2.0

	print(a * b)
	# output: [ 2.  4.  6.]

	print(a + b)
	# output: [ 3.  4.  5.]

	print(a * a)
	# output: [ 1.  4.  9.]

	In many ways NumPy can be used as a drop-in replacement for the math module.

	import numpy, math
	a = numpy.linspace(-math.pi, math.pi, 8)
	numpy.sin(a)

Vectorised operations
	# brute force using a for loop
	arr = numpy.arange(1000)
	dif = numpy.zeros(999, int)
	for i in range(1, len(arr)):
	dif[i-1] = arr[i] - arr[i-1]

	can be re-written as a vectorised operation:

	# vectorised operation
	arr = numpy.arange(1000)
	dif = arr[1:] - arr[:-1]

Array manipulation and broadcasting:
      Reshape:
      a = numpy.array([[1, 2, 3], [4, 5, 6]])

      print(a.shape)
      # output: (2,3)

      b = a.reshape(3,2)

      print(b)
      # output: [[1 2],
      #          [3 4],
      #          [5 6]]

      method called ravel() to flatten an array into 1D
      c = a.ravel()

      Concatenate
      a = numpy.array([[1, 2, 3], [4, 5, 6]])
      b = numpy.array([[7, 8, 9], [10, 11, 12]])
      c = numpy.concatenate((a, b))

      print(c)
      # output: [[ 1  2  3],
           	[ 4  5  6],
 		[ 7  8  9],
           	[10 11 12]]

      One can also specify the axis (dimension) along which the arrays will be joined:

      d = numpy.concatenate((a, b), axis=1)

      print(d)
      # output: [[ 1  2  3  7  8  9],
           	[ 4  5  6 10 11 12]]

      Split
      a = numpy.array([[1, 2, 3], [4, 5, 6]])
      x = numpy.split(a, 2)
      y = numpy.split(a, 3, axis=1)

      Broadcasting
      (E.g. when we multiply array with scalar)

      Example: A 3x2 array multiplied by a 1x2 array:
      a = numpy.arange(6).reshape(3,2)
      b = numpy.array([7,11], float).reshape(1,2)

      print(a)
      # output: [[0 1],
      #          [2 3],
      #          [4 5]]

      print(b)
      # output: [[ 7.  11.]]

      c = a * b

      print(c)
      # output: [[  0.  11.],
      #          [ 14.  33.],
      #          [ 28.  55.]]

# Loading data from txt files

  xy = numpy.loadtxt('xy-coordinates.dat')
  From two collumns txt file:
  x   y
  0.1 0.1
  -2 nan

  args = {
       'header': 'XY coordinates',
         'fmt': '%7.3f',
	   'delimiter': ','
  }
  numpy.savetxt('output.dat', xy, **args)

  # It is also possible to save binaries
  numpy.save('a_bin',a)

# Random numbers
 
   - random: uniform random numbers
   - normal: normal distribution
   - choice: random sample from given array

   a = numpy.random.random((2,2))
   b = numpy.random.choice(numpy.arange(4), 10)

# Linear algebra and polynomials

  dot, vdot
  linalg.eig, linalg.eigvals
  linalg.solve
  linalg.inv

  A = numpy.array(((2, 1), (1, 3)))
  B = numpy.array(((-2, 4.2), (4.2, 6)))

  C = numpy.dot(A, B)
  b = numpy.array((1, 2))

  print(C)
  # output:
  #   [[  0.2  14.4]
  #    [ 10.6  22.2]]

  print(b)
  # output: [1 2]

  # solve C x = b
  x = numpy.linalg.solve(C, b)

  print(x)
  # output: [ 0.04453441  0.06882591]

# Polynomials
  # f(x) = x^2 + random noise (between 0,1)
  x = numpy.linspace(-4, 4, 7)
  f = x**2 + numpy.random.random(x.shape)

  p = numpy.polyfit(x, f, 2)

  print(p)
  # output: [ 0.96869003  -0.01157275  0.69352514]
  #   f(x) =  p[0] * x^2 + p[1] * x  + p[2]


# Interesting information on indexing and numpy arrays characteristics

This is fast and creates the copy of the array -
finally I remembered how it was in GeoModel yet:

>>> a
    array([[ 2.3,  2.1],
       [ 4.2,  1.2]])

>>> ind = numpy.array([[0,1],[2,3]])
>>> a.flatten()[ind]
array([[ 2.3,  2.1],
       [ 4.2,  1.2]])

n addition to slicing NumPy allows indexing also
with integer arrays or Boolean masks:


     a = np.arange(0.0, 1.0, 0.1)
     ind = np.array([1, 1, 0, 4])
     b = a[ind] # b = array([0.1, 0.1, 0. , 0.4])

     m = a > 0.5
     b = a[m] # b = array([0.6, 0.7, 0.8, 0.9])

In these cases b cannot be created just by modifying the strides,
so b will hold a copy of the data in a.
Now, modifications of b are not affecting a.

>>> a.flags
  C_CONTIGUOUS : True
  F_CONTIGUOUS : False
  OWNDATA : True
  WRITEABLE : True
  ALIGNED : True
  UPDATEIFCOPY : False
>>> a.strides
(16, 8)
>>> a
array([[ 2.3,  2.1],
       [ 4.2,  1.2]])
>>> a.itemsize
8
>>> a.data
<memory at 0x7f535a528480>
>>> a.__array_interface__
{'data': (12984000, False), 'strides': None, 'descr': [('', '<f8')], 'typestr': '<f8', 'shape': (2, 2), 'version': 3}



